import numpy as np
from sentence_transformers import SentenceTransformer
from pypdf import PdfReader
from PIL import Image
import pytesseract
import os
import ollama
from faster_whisper import WhisperModel
import cv2

# -------------------------
# LOAD EMBEDDING MODEL
# -------------------------
model = SentenceTransformer("all-MiniLM-L6-v2")

# -------------------------
# LOAD TEXT FILE
# ------------------------- 
def load_txt(path):
    with open(path, "r", encoding="utf-8") as f:
        return f.read()

# -------------------------
# LOAD PDF FILE
# -------------------------
def load_pdf(path):
    reader = PdfReader(path)
    text = ""
    for page in reader.pages:
        text += page.extract_text()
    return text

# -------------------------
# LOAD IMAGE (OCR)
# -------------------------
def load_image(path):
    img = Image.open(path)
    return pytesseract.image_to_string(img)

# -------------------------
# LOAD AUDIO MODEL
# -------------------------
audio_model = WhisperModel("base", device="cpu", compute_type="int8")

# -------------------------
# LOAD AUDIO (SPEECH → TEXT)
# -------------------------
def load_audio(path):
    segments, _ = audio_model.transcribe(path)
    return " ".join(segment.text for segment in segments)

# -------------------------
# VIDEO FRAME OCR
# -------------------------
def ocr_keyframes(video_path, every_seconds=5, max_frames=12):
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS) or 30
    step = max(1, int(fps * every_seconds))

    texts = []
    frame_idx = 0
    grabbed = 0

    while cap.isOpened():
        ok, frame = cap.read()
        if not ok:
            break

        if frame_idx % step == 0:
            img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            txt = pytesseract.image_to_string(img).strip()
            if txt:
                texts.append(txt)
            grabbed += 1
            if grabbed >= max_frames:
                break

        frame_idx += 1

    cap.release()
    return "\n".join(texts)

# -------------------------
# LOAD VIDEO (AUDIO + OPTIONAL FRAME OCR)
# -------------------------
def load_video(path, do_frame_ocr=True):
    parts = []

    # 1) Audio transcript (best signal)
    parts.append(load_audio(path))

    # 2) On-screen text (optional, but great for slides/captions)
    if do_frame_ocr:
        frame_text = ocr_keyframes(path, every_seconds=5, max_frames=12)
        if frame_text.strip():
            parts.append(frame_text)

    return "\n".join(p for p in parts if p and p.strip())

# -------------------------
# LOAD ALL DATA FORMATS
# -------------------------
def load_all_data(folder="data"):
    all_text = ""
    for file in os.listdir(folder):
        path = os.path.join(folder, file)

        if file.endswith(".txt"):
            all_text += load_txt(path)
        elif file.endswith(".pdf"):
            all_text += load_pdf(path)
        elif file.endswith(".png") or file.endswith(".jpg"):
            all_text += load_image(path)
        elif file.endswith((".wav", ".mp3", ".m4a", ".flac", ".ogg")):
            print(f"Transcribing audio: {file}")
            all_text += load_audio(path)
        elif file.endswith((".mp4", ".mov", ".mkv", ".avi", ".webm")):
            print(f"Processing video: {file}")
            all_text += f"\n\n--- SOURCE: {file} (video) ---\n\n"
            all_text += load_video(path, do_frame_ocr=True)

    return all_text

# -------------------------
# SPLIT INTO CHUNKS
# -------------------------
def split_into_chunks(text, max_chars=500, overlap=100):
    chunks = []
    start = 0
    while start < len(text):
        end = start + max_chars
        chunk = text[start:end]
        chunks.append(chunk.strip())
        start = end - overlap
    return [c for c in chunks if c]

# -------------------------
# EMBEDDINGS
# -------------------------
def embed_texts(texts):
    return model.encode(texts, convert_to_numpy=True)

# -------------------------
# VECTOR DATABASE
# -------------------------
class VectorDB:
    def __init__(self, texts, embeddings):
        self.texts = texts
        self.embeddings = embeddings
        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
        self.normed_embeddings = embeddings / norms

    def search(self, query, k=3):
        q_emb = embed_texts([query])[0]
        q_emb = q_emb / np.linalg.norm(q_emb)

        sims = np.dot(self.normed_embeddings, q_emb)
        topk_idx = np.argsort(-sims)[:k]

        return [self.texts[i] for i in topk_idx]

# -------------------------
# PROMPT TEMPLATE (RAG)
# -------------------------
def build_prompt(context, user_question):
    return f"""
You are an assistant. Answer ONLY using the context below.

Context:
{context}

Question:
{user_question}

Answer:
"""

# -------------------------
# LLM CALL
# -------------------------
def ask_llm(prompt):
    response = ollama.chat(
        model="llama3",
        messages=[{"role": "user", "content": prompt}]
    )
    return response["message"]["content"]

# -------------------------
# MAIN RAG PIPELINE
# -------------------------
def main():
    print("Loading multi-format dataset...")
    raw_text = load_all_data("data")

    chunks = split_into_chunks(raw_text)
    embeddings = embed_texts(chunks)
    vdb = VectorDB(chunks, embeddings)

    print("RAG system ready.\n")

    while True:
        query = input("Ask a question (or exit): ")
        if query.lower() == "exit":
            break

        # ✅ WITHOUT RETRIEVAL (FAIL CASE)
        print("\n--- LLM Without Retrieval ---")
        plain_answer = ask_llm(query)
        print(plain_answer)

        # ✅ WITH RAG
        retrieved = vdb.search(query, k=3)
        context = "\n".join(retrieved)

        rag_prompt = build_prompt(context, query)

        print("\n--- LLM With RAG ---")
        rag_answer = ask_llm(rag_prompt)
        print(rag_answer)

# -------------------------
if __name__ == "__main__":
    main()
